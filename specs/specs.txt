================================================================================
StaffAI - Project Specifications
================================================================================

OVERVIEW
================================================================================

StaffAI is a production-ready Discord bot powered by Large Language Model (LLM)
technology. It provides intelligent, context-aware conversational AI through
the LiteLLM universal proxy, featuring structured JSON output, MCP (Model 
Context Protocol) tool calling, and advanced conversation management with
time-based context decay.

VERSION: 1.0.0
PYTHON: 3.8+
PRIMARY LANGUAGE: Spanish (with English support)

================================================================================
SECTION 1: AI CAPABILITIES
================================================================================

1.1 LLM INTEGRATION
--------------------------------------------------------------------------------
Provider Gateway:
- LiteLLM Proxy integration for 100+ LLM provider support
- Unified API for OpenAI, Anthropic, Google, Azure, local models (Ollama/LM Studio)
- Hot-swappable model configuration without code changes
- Fallback support for provider redundancy

Structured Output:
- JSON Schema enforcement (response_schema.json)
- Zero-parsing-error guarantee through schema validation
- Response types: text, url, gif, latex, code, output
- Consistent field structure: type, response (<=30 words), data

Temperature Configuration:
- Configurable temperature per model
- GPT-5 models forced to temperature=1.0 for compliance
- Default: 0.7

1.2 MODEL CONTEXT PROTOCOL (MCP) TOOL CALLING
--------------------------------------------------------------------------------
Architecture:
- Dynamic tool discovery from HTTP MCP servers
- FastMCP client with http-streamable transport (not SSE)
- Automatic schema conversion: FastMCP format -> OpenAI function calling format
- Session-level tool caching at bot startup

Tool Execution Flow:
1. Tools loaded once at bot initialization (setup_hook)
2. User message analyzed for tool keywords
3. LLM decides whether to call tools
4. Bot executes tool via MCP server
5. Results returned to LLM for final response

Three-Path Response Handling:
- Path A: Tools used -> Tool execution -> Structured response
- Path B: Tools available but ignored -> Second LLM call with structured output
- Path C: No tools -> Direct structured response

Error Resilience:
- 10-second timeout per MCP server
- Failed servers tracked and retried after 15 minutes
- Partial server failure doesn't block entire bot
- Clean error logging (no stack traces to users)

Tool Keyword Detection:
- gif, image, picture, foto, imagen
- run, execute, ejecuta, corre
- cve, vulnerability, vulnerabilidad
- weather, clima, tiempo
- youtube, video
- search, busca, find

1.3 CONTEXT MANAGEMENT
--------------------------------------------------------------------------------
Per-User/Per-Channel Memory:
- Separate conversation threads for each user in each channel
- Redis-backed persistence survives bot restarts
- Configurable max history messages (default: 20)

Time-Based Context Decay:
- CONTEXT_HISTORY_TTL_SECONDS: Entire conversation expiry (default: 1800s/30min)
- CONTEXT_MESSAGE_MAX_AGE_SECONDS: Individual message age limit (default: 1800s/30min)
- TTL refreshed on each interaction
- Stale messages automatically filtered

Context Injection Scenarios:
- Scenario 1 (Mention): Use existing user-bot history only
- Scenario 2 (Reply to Bot): Use existing user-bot history only
- Scenario 3 (Tag Bot on Reply to Other User): Inject referenced message context
- Scenario 4 (Random Response): Fetch last 5-10 channel messages for context

1.4 DYNAMIC PRESENCE GENERATION
--------------------------------------------------------------------------------
LLM-Generated Status:
- Bot activity text generated by LLM itself
- Periodic updates configurable (default: 5 minutes)
- Context-aware status reflects "mood" or activity

Activity Types:
- Playing (game title or activity)
- Listening to (song, podcast, sound)
- Watching (show, movie, video)
- Custom (general status message)

Time-Based Scheduling:
- Enable/disable by hours UTC
- Enable/disable by days of week
- Idle status outside active hours

1.5 MULTI-MODAL RESPONSE RENDERING
--------------------------------------------------------------------------------
Response Types:
- text: Plain conversation messages
- url: Links with contextual description
- gif: Animated GIF URLs (Tenor/Giphy integration via MCP)
- latex: Mathematical formulas (LaTeX to PNG rendering)
- code: Syntax-highlighted code blocks
- output: Command execution results

LaTeX Rendering:
- Formulas rendered as PNG images via latex2image.joeraut.com
- Base64 fallback for large formulas
- Embedded in Discord messages

1.6 LANGUAGE DETECTION AND RESPONSE
--------------------------------------------------------------------------------
Automatic Language Matching:
- Bot responds in same language as user message
- Spanish as primary language (most conversations)
- English fully supported
- Language detection in personality prompt

================================================================================
SECTION 2: PROTECTION AND RATE LIMITING
================================================================================

2.1 DUAL-TIER RATE LIMITING
--------------------------------------------------------------------------------
Message Count Rate Limiting:
- Max messages per user per time window
- Sliding window implementation via Redis lists
- Key: msg_rl:{guild_id}:{user_id}

Token Consumption Rate Limiting:
- Max LLM tokens consumed per user per window
- Prevents expensive API abuse
- Key: token_rl:{guild_id}:{user_id}
- Format: timestamp:tokens entries

Both Limits:
- Tracked separately and independently enforced
- Configurable window size (default: 60 seconds)
- Configurable limits (default: 15 messages, 20000 tokens)

2.2 AUTOMATIC RESTRICTION SYSTEM
--------------------------------------------------------------------------------
Restriction Application:
- Users exceeding limits assigned "Restricted User" role
- Restricted users limited to designated channel only
- Public notification system (suppressed for random responses)

Auto-Expiry:
- Background task checks restriction expiry periodically
- Key: restricted_until:{guild_id}:{user_id}
- Configurable duration (default: 86400s/24h)
- Role automatically removed when time expires

2.3 ROLE-BASED ACCESS CONTROL
--------------------------------------------------------------------------------
Super User Roles:
- Bypass all rate limits
- Access to /tokenstats command
- Configurable via SUPER_ROLE_IDS

Ignored Roles:
- Bot completely ignores messages from these roles
- Useful for other bots or specific user groups
- Configurable via IGNORED_ROLE_IDS

================================================================================
SECTION 3: TOKEN CONSUMPTION ANALYTICS
================================================================================

3.1 AUTOMATIC TOKEN TRACKING
--------------------------------------------------------------------------------
Data Captured:
- Total tokens (all-time cumulative)
- Daily tokens (7-day retention)
- Usage log (last 1000 interactions)

Storage:
- Redis-backed with automatic expiry
- Key patterns:
  - token_stats:total:{guild_id}:{user_id}
  - token_stats:daily:{guild_id}:{date}:{user_id}
  - token_stats:log:{guild_id}:{user_id}

3.2 ADMIN COMMAND: /TOKENSTATS
--------------------------------------------------------------------------------
Access: Super user roles only
Format: Ephemeral embed (visible only to command issuer)
Information Displayed:
- Total tokens consumed
- Today's token usage
- Estimated cost (model-specific pricing)
- Recent activity log (last 10 interactions)
- User avatar and username

3.3 AUTOMATED REPORTS
--------------------------------------------------------------------------------
Delivery: Configurable channel and interval
Content:
- Top N users by token consumption
- Ranking with medals (gold, silver, bronze)
- Total and daily tokens with costs
- Grand totals across all users

Model Pricing Support:
- Gemini (flash, flash-lite, pro)
- OpenAI (gpt-5, gpt-4o, gpt-4-turbo, gpt-3.5-turbo)
- XAI (grok-3-mini)
- Anthropic (claude-3-5-sonnet, claude-3-haiku, claude-3-opus)

================================================================================
SECTION 4: BOT BEHAVIOR AND ENGAGEMENT
================================================================================

4.1 ENGAGEMENT TRIGGERS
--------------------------------------------------------------------------------
Trigger Types:
- @Mention: Direct engagement when bot is tagged
- Reply: Automatic response when replying to bot messages
- Random: Probability-based conversation joining

Random Response Behavior:
- RESPONSE_CHANCE: Initial trigger probability (default: 5%)
- RANDOM_RESPONSE_DELIVERY_CHANCE: Secondary filter (default: 30%)
- Dual-probability prevents excessive random responses

4.2 RANDOM RESPONSE CONTEXT AWARENESS
--------------------------------------------------------------------------------
Channel Context Fetching:
- Last 5-10 messages from channel
- Only messages within CONTEXT_MESSAGE_MAX_AGE_SECONDS
- No user-bot history used (fresh interjection)

Context Formatting:
- Chronological order (oldest to newest)
- User attribution for each message
- Deduplication of repeated messages

4.3 MESSAGE DEDUPLICATION
--------------------------------------------------------------------------------
Dual-Layer Deduplication:
- Redis-based (atomic, distributed)
- In-memory fallback (always active)
- Key: processed_msg:{channel}_{author}_{reference}_{content_hash}
- 60-second TTL

================================================================================
SECTION 5: DATA STRUCTURES (REDIS)
================================================================================

5.1 CONVERSATION HISTORY
--------------------------------------------------------------------------------
Key: discord_context:{user_id}:{channel_id}
Type: JSON string
Value: Array of message objects with role, content, timestamp
TTL: CONTEXT_HISTORY_TTL_SECONDS (refreshed on save)
Filtering: Messages older than CONTEXT_MESSAGE_MAX_AGE_SECONDS removed

5.2 RATE LIMITING
--------------------------------------------------------------------------------
Message Count:
  Key: msg_rl:{guild_id}:{user_id}
  Type: Redis List
  Value: Timestamps of recent messages
  
Token Consumption:
  Key: token_rl:{guild_id}:{user_id}
  Type: Redis List
  Value: "timestamp:tokens" entries

5.3 RESTRICTIONS
--------------------------------------------------------------------------------
Key: restricted_until:{guild_id}:{user_id}
Type: String
Value: Unix timestamp of restriction expiry

5.4 MESSAGE DEDUPLICATION
--------------------------------------------------------------------------------
Key: processed_msg:{message_key}
Type: Set
Value: "1" (presence marker)
TTL: 60 seconds

5.5 TOKEN STATISTICS
--------------------------------------------------------------------------------
Total: token_stats:total:{guild_id}:{user_id}
Daily: token_stats:daily:{guild_id}:{date}:{user_id}
Log: token_stats:log:{guild_id}:{user_id} (sorted set)

================================================================================
SECTION 6: EXTERNAL DEPENDENCIES
================================================================================

6.1 REQUIRED SERVICES
--------------------------------------------------------------------------------
LiteLLM Proxy:
- Universal LLM gateway
- Must be accessible at LITELLM_API_URL
- Handles all LLM provider communication

Redis 5.0+:
- Conversation persistence
- Rate limiting
- Restriction tracking
- Token statistics

Discord API:
- Bot platform
- Requires Server Members Intent (Privileged)
- Requires Message Content Intent (Privileged)
- Permissions: View Channel, Send Messages, Manage Roles

6.2 OPTIONAL SERVICES
--------------------------------------------------------------------------------
MCP Servers:
- Tool calling via Model Context Protocol
- HTTP streamable transport
- Configured via MCP_SERVERS environment variable

6.3 PYTHON DEPENDENCIES
--------------------------------------------------------------------------------
Core:
- discord.py >= 2.3.0
- python-dotenv >= 1.0.0
- litellm (latest)
- openai >= 1.0.0
- redis >= 5.0.0

MCP Support:
- fastmcp >= 2.0.0
- aiohttp >= 3.9.0

Utilities:
- psutil (system stats)
- rich (formatted output)

================================================================================
SECTION 7: ARCHITECTURE OVERVIEW
================================================================================

7.1 COMPONENT STRUCTURE
--------------------------------------------------------------------------------
Entry Point: main.py
- Configuration loading and validation
- Environment variable parsing
- Docker secrets support
- Bot instantiation

Core Bot: bot.py
- AIBot class extending discord.ext.commands.Bot
- Redis client initialization
- LiteLLM client initialization
- Cog registration in setup_hook()

Cogs:
- message_cog.py: Discord event handling, response rendering
- activity_cog.py: Dynamic presence generation
- stats_cog.py: Token tracking and reporting

Utilities:
- utils/litellm_client.py: LiteLLM proxy client, MCP tools, context management
- utils/message_handler.py: Message processing, rate limiting, engagement logic

7.2 DATA FLOW
--------------------------------------------------------------------------------
1. Discord Message -> message_cog.py (event capture)
2. message_cog.py -> message_handler.py (should respond? rate limit?)
3. message_handler.py -> litellm_client.py (build context, call LLM)
4. litellm_client.py -> LiteLLM Proxy -> LLM Provider
5. LLM Response -> Structured JSON parsing
6. message_cog.py (render response based on type)

7.3 STRUCTURED OUTPUT SCHEMA
--------------------------------------------------------------------------------
File: response_schema.json
Format:
{
  "type": "text|url|gif|latex|code|output",
  "response": "User-facing message (<=30 words)",
  "data": "Type-specific content"
}

================================================================================
SECTION 8: DEPLOYMENT
================================================================================

8.1 DOCKER
--------------------------------------------------------------------------------
- Dockerfile provided
- docker-compose.yaml for multi-container setup
- External networks: bot, dbnet
- Secrets mounted at /run/secrets/

8.2 DIRECT PYTHON
--------------------------------------------------------------------------------
- pip install -r requirements.txt
- Configure .env file
- Ensure Redis and LiteLLM accessible
- python main.py

================================================================================
SECTION 9: SECURITY
================================================================================

9.1 SECRETS MANAGEMENT
--------------------------------------------------------------------------------
- Environment variables via .env
- Docker secrets for production (/run/secrets/)
- .env excluded from git via .gitignore

9.2 INPUT VALIDATION
--------------------------------------------------------------------------------
- All configuration validated at startup
- Fail-fast on critical errors
- Comprehensive logging

9.3 RATE LIMITING
--------------------------------------------------------------------------------
- Prevents API abuse
- Cost protection
- Automatic restriction for abusers

================================================================================
SECTION 10: MONITORING
================================================================================

10.1 LOGGING
--------------------------------------------------------------------------------
- Configurable log level (LOG_LEVEL)
- File logging: bot.log
- Console logging: stdout
- Structured format with timestamps

10.2 REDIS DEBUGGING
--------------------------------------------------------------------------------
- All keys documented
- redis-cli for inspection
- TTL visibility

10.3 HEALTH INDICATORS
--------------------------------------------------------------------------------
- Startup validation
- Redis connectivity test
- LiteLLM connectivity test

================================================================================
END OF SPECIFICATIONS
================================================================================
