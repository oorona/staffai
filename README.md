# StaffAI - Advanced Discord AI Bot

A production-ready Discord bot powered by state-of-the-art LLM technology via **LiteLLM proxy**, featuring **structured output**, **MCP tool calling**, and intelligent conversation management.

---

## ğŸ¤– AI Capabilities (Core Features)

### **Advanced LLM Integration**
- **LiteLLM Proxy**: Universal gateway to 100+ LLM providers (OpenAI, Anthropic, Google, local models via Ollama/LM Studio, etc.)
- **Structured Output**: Guaranteed JSON responses via schema validation - no more parsing errors or malformed responses
- **MCP (Model Context Protocol) Tool Calling**: Extensible function calling system for dynamic tool integration
- **Multi-Modal Responses**: The bot can respond with:
  - ğŸ“ **Text** - Natural conversation
  - ğŸ”— **URLs** - Links with context
  - ğŸ¬ **GIFs** - Animated responses
  - ğŸ“ **LaTeX** - Mathematical formulas (rendered as images)
  - ğŸ’» **Code** - Syntax-highlighted code blocks with execution output
  - ğŸ“Š **Output** - Command execution results

### **Intelligent Context Management**
- **Per-User, Per-Channel Memory**: Maintains separate conversation threads for each user in each channel
- **Time-Based Context Decay**: Automatically prunes old messages based on configurable TTL (default: 30 minutes)
  - `CONTEXT_HISTORY_TTL_SECONDS`: How long entire conversation history persists
  - `CONTEXT_MESSAGE_MAX_AGE_SECONDS`: Maximum age for individual messages in context
- **Context Injection**: When users reply to the bot's messages in other threads, relevant context is intelligently merged
- **Redis-Backed Persistence**: Conversation history survives bot restarts
- **Smart Channel Context**: For random responses, bot fetches recent channel messages to maintain conversational coherence

### **Response Generation Modes**
1. **@Mention Responses**: Direct engagement when bot is tagged
2. **Reply Responses**: Automatic response to replies to bot messages
3. **Random Interventions**: Configurable probability (`RESPONSE_CHANCE`) to randomly join conversations
   - Analyzes recent channel context (last few messages)
   - Generates contextually relevant interjections
   - Maintains conversation flow naturally

### **Dynamic Bot Presence**
- **LLM-Generated Status**: Bot's presence/activity is periodically generated by the LLM itself
- **Activity Type Rotation**: Cycles through Playing/Listening/Watching/Custom statuses
- **Time-Based Scheduling**: Configure active hours and days for status updates
- **Contextual Activities**: Status reflects current bot "mood" or server activity

---

## ğŸ›¡ï¸ Protection & Rate Limiting

### **Dual-Tier Rate Limiting**
- **Message Count Limiting**: Max messages per user per time window
- **Token Consumption Limiting**: Max LLM tokens consumed per user (prevents expensive abuse)
- Both limits tracked separately in Redis with sliding window

### **Automatic Restriction System**
- Users exceeding limits are assigned a "Restricted User" role
- Restricted users can only interact in a designated channel
- Auto-expiry system removes restrictions after configurable duration
- Public notification system (suppressed for random responses)

### **Exemption System**
- Configure "super user" roles that bypass all limits
- Global ignore list for specific roles (bot won't respond at all)

---

## ğŸ—ï¸ Architecture

### **Core Components**
- `main.py` - Configuration loading, validation, bot initialization
- `bot.py` - `AIBot` class, Redis/LiteLLM client wiring, Cog loading
- `utils/litellm_client.py` - LiteLLM proxy client with structured output and MCP tools
- `utils/message_handler.py` - Message processing logic, engagement decisions, rate limiting
- `cogs/message_cog.py` - Discord event handling, response rendering
- `cogs/activity_cog.py` - Dynamic presence generation

### **Data Flow**
```
Discord Message
    â†“
message_cog.py (event capture)
    â†“
message_handler.py (should respond? rate limit check)
    â†“
litellm_client.py (build context, call LLM with schema)
    â†“
Structured JSON Response (type/response/data)
    â†“
message_cog.py (render to Discord based on type)
```

### **Structured Output Schema**
All LLM responses follow a strict JSON schema:
```json
{
  "type": "text|url|gif|latex|code|output",
  "response": "User-facing message (â‰¤30 words, personality-driven)",
  "data": "Type-specific content (URL, formula, code, etc.)"
}
```

**Example Responses:**
```json
{"type": "text", "response": "Use list.sort(). If that's too hard, maybe knitting is more your speed."}
{"type": "latex", "response": "Here's basic calculus. Try to keep up.", "data": "\\int e^x dx = e^x + C"}
{"type": "code", "response": "Here's how competent devs do it:", "data": "def fib(n):\n    return n if n < 2 else fib(n-1) + fib(n-2)"}
```

---

## ğŸ“‹ Prerequisites

- **Python 3.8+**
- **Discord Bot** with:
  - `Server Members Intent` (Privileged)
  - `Message Content Intent` (Privileged)
  - Permissions: `View Channel`, `Send Messages`, `Manage Roles`
- **LiteLLM Proxy** running and accessible
- **Redis 5.0+** for conversation persistence and rate limiting
- **Discord Role Setup**: Manually create "Restricted User" role (bot's role must be higher in hierarchy)

---

## âš™ï¸ Configuration

Create a `.env` file:

```env
# Discord
DISCORD_BOT_TOKEN=your_bot_token_here

# Logging
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# LiteLLM Configuration
LITELLM_API_URL=http://localhost:4000  # Your LiteLLM proxy URL
LITELLM_MODEL=gpt-4o-mini               # Model identifier
LITELLM_API_KEY=sk-1234                 # API key (often dummy for local proxies)

# MCP Servers (comma-separated URLs)
MCP_SERVERS=http://mcp-server-1:8000,http://mcp-server-2:8000

# Bot Behavior
RESPONSE_CHANCE=0.05           # 5% chance to randomly respond to messages
RANDOM_RESPONSE_DELIVERY_CHANCE=0.3  # 30% chance to actually send random responses (secondary filter)
MAX_HISTORY_PER_USER=20        # Max messages in context per user/channel

# Context Decay (seconds)
CONTEXT_HISTORY_TTL_SECONDS=1800        # 30 min - entire history expiry
CONTEXT_MESSAGE_MAX_AGE_SECONDS=1800    # 30 min - individual message age limit

# Redis
REDIS_HOST=localhost  # Use 'redis' for docker-compose
REDIS_PORT=6379
REDIS_DB=0
# REDIS_PASSWORD=your_password  # Uncomment if needed

# Rate Limiting
RATE_LIMIT_COUNT=15                # Max messages per user per window
RATE_LIMIT_WINDOW_SECONDS=60       # Window size in seconds
TOKEN_RATE_LIMIT_COUNT=20000       # Max tokens per user per window

# Restriction System
RESTRICTED_USER_ROLE_ID=your_role_id_here     # Role assigned when limits exceeded
RESTRICTED_CHANNEL_ID=your_channel_id_here    # Where restricted users can interact
RESTRICTION_DURATION_SECONDS=86400            # 24 hours
RESTRICTION_CHECK_INTERVAL_SECONDS=300        # Check every 5 minutes

# Notification Templates
RATE_LIMIT_MESSAGE_USER="You've sent messages too frequently. Please use <#{channel_id}> for bot interactions."
RESTRICTED_CHANNEL_MESSAGE_USER="As a restricted user, please use <#{channel_id}> for bot interactions."

# Role-Based Access
SUPER_ROLE_IDS=          # Comma-separated role IDs that bypass rate limits
IGNORED_ROLE_IDS=        # Comma-separated role IDs the bot completely ignores

# Activity/Presence
ACTIVITY_UPDATE_INTERVAL_SECONDS=300      # Update status every 5 minutes
ACTIVITY_SCHEDULE_ENABLED=False           # Enable time-based scheduling
ACTIVITY_ACTIVE_START_HOUR_UTC=0
ACTIVITY_ACTIVE_END_HOUR_UTC=23
ACTIVITY_ACTIVE_DAYS_UTC=0,1,2,3,4,5,6   # Days of week (0=Monday, 6=Sunday)
```

---

## ğŸš€ Installation & Running

### Method 1: Direct Python

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Configure `.env`** (see configuration section above)

3. **Ensure services running:**
   - LiteLLM proxy accessible
   - Redis server running

4. **Create prompt files:**
   ```bash
   mkdir -p utils/prompts
   # Add personality_prompt.txt and base_activity_system_prompt.txt
   ```

5. **Run:**
   ```bash
   python main.py
   ```

### Method 2: Docker Compose (Recommended)

1. **Create secret files:**
   ```bash
   # Create Discord bot token secret
   echo "YOUR_DISCORD_BOT_TOKEN" > secrets/discord_bot_token.txt
   
   # Create LiteLLM API key secret
   echo "YOUR_LITELLM_API_KEY" > secrets/litellm_api_key.txt
   
   # Secure the files
   chmod 600 secrets/*.txt
   ```

2. **Configure `.env`:**
   - Set `REDIS_HOST=redis` (docker-compose service name)
   - Set `LITELLM_API_URL` to your LiteLLM proxy
   - **Note:** `DISCORD_BOT_TOKEN` and `LITELLM_API_KEY` should use Docker secrets (not in .env)

3. **Build and run:**
   ```bash
   docker-compose up --build -d
   ```

4. **View logs:**
   ```bash
   docker-compose logs -f staffai
   ```

5. **Stop:**
   ```bash
   docker-compose down
   ```

**Security Note:** Docker secrets are mounted at `/run/secrets/` inside the container. The bot automatically reads from there in production, falling back to environment variables for local development.

---

## ğŸ“ Project Structure

```
staffai/
â”œâ”€â”€ main.py                      # Entry point, config validation, bot initialization
â”œâ”€â”€ bot.py                       # AIBot class, Redis/LiteLLM wiring, Cog loader
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ Dockerfile                   # Docker image build instructions
â”œâ”€â”€ docker-compose.yaml          # Docker services (bot + optional Redis)
â”œâ”€â”€ response_schema.json         # Structured output JSON schema
â”œâ”€â”€ .env                         # Configuration (DO NOT COMMIT)
â”œâ”€â”€ bot.log                      # Application logs
â”‚
â”œâ”€â”€ cogs/
â”‚   â”œâ”€â”€ message_cog.py          # Discord event handling, response rendering
â”‚   â””â”€â”€ activity_cog.py          # Dynamic presence generation
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ litellm_client.py        # LiteLLM proxy client with structured output
â”‚   â”œâ”€â”€ message_handler.py       # Message processing, rate limiting, engagement
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ personality_prompt.txt         # Bot personality & response format
â”‚       â””â”€â”€ base_activity_system_prompt.txt # Activity generation prompt
â”‚
â””â”€â”€ secrets/
    â””â”€â”€ discord_bot_token.txt    # Docker secret (optional)
```

---

## ğŸ¯ Key Differentiators

### **Why StaffAI?**

1. **Production-Ready AI Integration**
   - Structured output eliminates parsing errors
   - Universal LLM support via LiteLLM (swap providers without code changes)
   - MCP tool system for extensibility

2. **Intelligent Behavior**
   - Context-aware random responses (analyzes recent channel messages)
   - Time-based conversation decay (no stale context)
   - Multi-threaded conversation tracking per user/channel

3. **Enterprise-Grade Protection**
   - Dual-tier rate limiting (messages + tokens)
   - Automatic restriction with expiry
   - Role-based exemptions and ignores

4. **Developer-Friendly**
   - Clean separation of concerns
   - Comprehensive logging
   - Docker-first deployment
   - Extensive configuration options

---

## ğŸ”§ Advanced Configuration

### **Random Response Context Window**
When the bot randomly responds, it fetches recent channel messages to understand context:
- Default: Last 5-10 messages
- Analyzes conversation flow
- Generates relevant interjection
- Maintains natural conversation rhythm

### **Structured Output Types**

| Type | Description | Data Field |
|------|-------------|------------|
| `text` | Plain conversation | N/A |
| `url` | Links with context | URL string |
| `gif` | Tenor/Giphy URLs | GIF URL |
| `latex` | Math formulas | LaTeX code |
| `code` | Code snippets | Code string |
| `output` | Execution results | Terminal output |

### **MCP Server Integration**
Add MCP servers via `MCP_SERVERS` env var (comma-separated):
```env
MCP_SERVERS=http://filesystem-mcp:8000,http://web-search-mcp:8001
```

The bot automatically:
- Discovers available tools from each server
- Sorts tools alphabetically (workaround for OpenWebUI bug)
- Passes tools to LLM in compatible format
- Executes tool calls and returns results

---

## ğŸ“Š Monitoring & Debugging

### **Logs**
- **Console**: Real-time output
- **File**: `bot.log` (persists across runs)
- **Level**: Set via `LOG_LEVEL` env var

### **Redis Keys** (for debugging)
```
discord_context:{user_id}:{channel_id}  # Conversation history
msg_rl:{guild_id}:{user_id}             # Message rate limit
token_rl:{guild_id}:{user_id}           # Token rate limit
restricted_until:{guild_id}:{user_id}   # Restriction expiry timestamp
```

### **Health Checks**
- Bot validates all critical config on startup
- Fails fast with detailed error messages
- Redis connectivity tested before accepting messages

---

## âš ï¸ Important Implementation Notes

### MCP Tool Calling Requirements
When modifying tool calling code, follow these **critical** rules (see `.github/copilot-instructions.md` and `specs/specs.txt` for details):

1. **Always use `tool_choice="auto"`** when passing tools to LLM
2. **Use fresh conversations** (no history) when tools are available
3. **Use raw message objects** (`response.choices[0].message`), never manual dicts
4. **Don't set `max_tokens`** - let models use their defaults
5. **Always set `timeout=60.0`** explicitly

**Breaking these rules causes**: Tools not being called, empty responses, or truncated JSON.

**Testing**: Run `demo/test_tool_calling.py` first to verify expected behavior before modifying bot code.

---

## ğŸ¤ Contributing

This bot is designed for extensibility:

1. **Add new response types**: Update `response_schema.json` and `message_cog.py` renderer
2. **Add new triggers**: Extend `message_handler.py` engagement logic
3. **Add new MCP servers**: Deploy server, add URL to env var
4. **Add new cogs**: Create in `cogs/`, register in `bot.py`

---

## ğŸ“„ License

[Your License Here]

---

## ğŸ™ Acknowledgments

- **LiteLLM** - Universal LLM gateway
- **Discord.py** - Discord API wrapper
- **FastMCP** - Model Context Protocol client
- **Redis** - In-memory data structure store
